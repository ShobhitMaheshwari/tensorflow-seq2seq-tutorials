{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "UNK = 2\n",
    "\n",
    "#change this to 10 for the original example to work\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "encoder_inputs2 = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length2 = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "# decoder_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='decoder_lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "encoder_inputs_embedded2 = tf.nn.embedding_lookup(embeddings, encoder_inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "encoder_cell = LSTMCell(encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('first'):\n",
    "    ((encoder_fw_outputs,\n",
    "      encoder_bw_outputs),\n",
    "     (encoder_fw_final_state,\n",
    "      encoder_bw_final_state)) = (\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded,\n",
    "                                        sequence_length=encoder_inputs_length,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "        )\n",
    "with tf.variable_scope('second'):\n",
    "    ((encoder_fw_outputs2,\n",
    "      encoder_bw_outputs2),\n",
    "     (encoder_fw_final_state2,\n",
    "      encoder_bw_final_state2)) = (\n",
    "        tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                        cell_bw=encoder_cell,\n",
    "                                        inputs=encoder_inputs_embedded2,\n",
    "                                        sequence_length=encoder_inputs_length2,\n",
    "                                        dtype=tf.float32, time_major=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'first/bidirectional_rnn/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'first/ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'first/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'first/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_fw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'first/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'first/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_bw_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_outputs2 = tf.concat((encoder_fw_outputs2, encoder_bw_outputs2), 2)\n",
    "\n",
    "encoder_final_state_c2 = tf.concat(\n",
    "    (encoder_fw_final_state2.c, encoder_bw_final_state2.c), 1)\n",
    "\n",
    "encoder_final_state_h2 = tf.concat(\n",
    "    (encoder_fw_final_state2.h, encoder_bw_final_state2.h), 1)\n",
    "\n",
    "encoder_final_state2 = LSTMStateTuple(\n",
    "    c=encoder_final_state_c2,\n",
    "    h=encoder_final_state_h2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_state = LSTMStateTuple(\n",
    "    c=tf.concat([encoder_final_state.c, encoder_final_state2.c], 1),\n",
    "    h=tf.concat([encoder_final_state.h, encoder_final_state2.h], 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_state.c.get_shape().as_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A fully connected layer between encoder_final_state.c (encoder_hidden_units) and decoder_hidden_units\n",
    "def multilayer_perceptron(x, w, b):\n",
    "    layer_1 = tf.add(tf.matmul(x, w), b)\n",
    "    return tf.nn.relu(layer_1)\n",
    "    \n",
    "with tf.variable_scope('c'):\n",
    "    wc = tf.Variable(tf.random_normal([combined_state.c.get_shape().as_list()[1], decoder_hidden_units]))\n",
    "    bc = tf.Variable(tf.random_normal([decoder_hidden_units]))\n",
    "    \n",
    "with tf.variable_scope('h'):\n",
    "    wh = tf.Variable(tf.random_normal([combined_state.h.get_shape().as_list()[1], decoder_hidden_units]))\n",
    "    bh = tf.Variable(tf.random_normal([decoder_hidden_units]))\n",
    "projected_state = tf.contrib.rnn.LSTMStateTuple(\n",
    "    c=multilayer_perceptron(combined_state.c, wc, bc),\n",
    "    h=multilayer_perceptron(combined_state.h, wh, bh),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = tf.maximum(encoder_inputs_length, encoder_inputs_length2) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert EOS == 1 and PAD == 0\n",
    "\n",
    "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = projected_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[3, 3, 6, 9, 2]\n",
      "[8, 3, 7, 3]\n",
      "[3, 4, 7, 8, 9]\n",
      "[8, 7, 6, 4, 9, 5]\n",
      "[8, 8, 5, 5]\n",
      "[2, 3, 2, 9]\n",
      "[3, 8, 5, 9, 8]\n",
      "[4, 2, 6, 8]\n",
      "[7, 6, 5]\n",
      "[2, 2, 2, 9, 3]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add(batch1, batch2):\n",
    "    targetSeq = []\n",
    "    for i in range(0, max(len(batch1), len(batch2))):\n",
    "        if i >=len(batch1):\n",
    "            targetSeq.append((int)(batch2[i]/2))\n",
    "        elif i >=len(batch2):\n",
    "            targetSeq.append((int)(batch1[i])/2)\n",
    "        else:\n",
    "            targetSeq.append((int)((batch1[i]+batch2[i])/2))\n",
    "    return targetSeq\n",
    "def next_feed():\n",
    "    batch1 = next(batches)\n",
    "    batch2 = next(batches)\n",
    "    encoder_inputs_1, encoder_input_lengths_1 = helpers.batch(batch1)\n",
    "    encoder_inputs_2, encoder_input_lengths_2 = helpers.batch(batch2)\n",
    "    \n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "    [add(x,y)  + [EOS] + [PAD] * 2 for x,y in zip(batch1, batch2)]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_1,\n",
    "        encoder_inputs_length: encoder_input_lengths_1,\n",
    "        encoder_inputs2: encoder_inputs_2,\n",
    "        encoder_inputs_length2: encoder_input_lengths_2,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 2.5091679096221924\n",
      "  sample 1:\n",
      "    input     > [5 4 9 3 4 8 8 0]\n",
      "    input     > [4 3 7 9 0 0 0 0]\n",
      "    predicted > [4 4 4 4 2 4 4 2 4 2 0]\n",
      "  sample 2:\n",
      "    input     > [4 8 7 4 0 0 0 0]\n",
      "    input     > [5 9 7 3 6 0 0 0]\n",
      "    predicted > [4 4 4 4 2 4 4 2 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 4 9 9 3 7 4 0]\n",
      "    input     > [9 9 9 5 8 6 7 6]\n",
      "    predicted > [4 4 4 4 4 2 2 4 4 2 4]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 0.9064642786979675\n",
      "  sample 1:\n",
      "    input     > [5 3 4 3 4 6 0 0]\n",
      "    input     > [3 2 4 4 0 0 0 0]\n",
      "    predicted > [4 3 3 3 3 3 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [8 4 7 7 0 0 0 0]\n",
      "    input     > [5 5 2 0 0 0 0 0]\n",
      "    predicted > [6 5 4 3 1 0 0 0 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [3 3 3 8 2 0 0 0]\n",
      "    input     > [6 7 3 9 0 0 0 0]\n",
      "    predicted > [4 4 5 4 1 1 0 0 0 0 0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.6291081309318542\n",
      "  sample 1:\n",
      "    input     > [2 5 3 0 0 0 0 0]\n",
      "    input     > [9 2 9 9 2 0 0 0]\n",
      "    predicted > [5 4 4 4 1 1 0 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [2 3 7 4 0 0 0 0]\n",
      "    input     > [8 6 2 9 9 6 4 0]\n",
      "    predicted > [5 4 4 5 4 2 2 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [5 7 4 9 5 0 0 0]\n",
      "    input     > [2 3 6 8 0 0 0 0]\n",
      "    predicted > [3 5 6 8 2 1 0 0 0 0 0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.4606470763683319\n",
      "  sample 1:\n",
      "    input     > [3 6 8 0 0 0 0 0]\n",
      "    input     > [9 6 6 2 9 5 0 0]\n",
      "    predicted > [6 6 7 2 2 2 1 0 0 0 0]\n",
      "  sample 2:\n",
      "    input     > [4 6 5 7 9 0 0 0]\n",
      "    input     > [6 3 3 5 4 7 3 0]\n",
      "    predicted > [5 4 4 6 5 3 1 1 0 0 0]\n",
      "  sample 3:\n",
      "    input     > [8 3 3 3 0 0 0 0]\n",
      "    input     > [7 9 6 0 0 0 0 0]\n",
      "    predicted > [7 6 4 1 1 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_track = []\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp1, inp2, pred) in enumerate(zip(fd[encoder_inputs].T, fd[encoder_inputs2].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp1))\n",
    "                print('    input     > {}'.format(inp2))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.4518 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb218b80b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert above stuff for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data, random, numpy as np\n",
    "train,test = data.readfnp()\n",
    "max_batches = 30001\n",
    "ran = np.random.randint(1, high=4, size=max_batches)\n",
    "idx = []\n",
    "for batch in range(max_batches):\n",
    "    idx.append(np.random.choice(len(train[ran[batch]]),size=100, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save this stuff \n",
    "import pickle\n",
    "pickle.dump( (ran, idx), open( \"randomstuff\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed2(i):\n",
    "    samples = data.getnextr(100, train, ran[i], idx[i])\n",
    "    batch1 = [x for x,y,z in samples]\n",
    "    batch2 = [y for x,y,z in samples]\n",
    "    encoder_inputs_1, encoder_input_lengths_1 = helpers.batch(batch1)\n",
    "    encoder_inputs_2, encoder_input_lengths_2 = helpers.batch(batch2)\n",
    "    \n",
    "    max_len = max(encoder_inputs_1.shape[0], encoder_inputs_2.shape[0])\n",
    "    encoder_inputs_1_r = np.zeros((max_len, 100), dtype=np.int32)\n",
    "    encoder_inputs_1_r[:encoder_inputs_1.shape[0], :encoder_inputs_1.shape[1]] = encoder_inputs_1\n",
    "    \n",
    "    encoder_inputs_2_r = np.zeros((max_len, 100), dtype=np.int32)\n",
    "    encoder_inputs_2_r[:encoder_inputs_2.shape[0], :encoder_inputs_2.shape[1]] = encoder_inputs_2\n",
    "    \n",
    "    decoder_targets_, oo = helpers.batch(\n",
    "    [z  + [EOS] + [PAD] * 2 for x,y,z in samples]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_1_r,\n",
    "        encoder_inputs_length: encoder_input_lengths_1,\n",
    "        encoder_inputs2: encoder_inputs_2_r,\n",
    "        encoder_inputs_length2: encoder_input_lengths_2,\n",
    "        decoder_targets: decoder_targets_,\n",
    "        decoder_lengths: oo\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "batch 0\n",
      "  minibatch loss: 10.728058815002441\n",
      "  sample 1:\n",
      "    input     > ['``', 'i', 'scratch', 'your', 'back', ',', 'you', 'scratch', 'mine', \"''\", 'is', 'what', 'he', 'said', '.', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['i', \"'m\", 'not', 'taking', 'care', 'of', 'merlot', ',', 'that', 'would', 'involve', 'me', 'seeing', 'him', 'more', 'than', 'once', '.', 'PAD']\n",
      "    predicted > ['psychic', 'twigs', 'twang', 'twigs', 'ytmnd', 'airing', 'shion', 'airing', 'katie', 'bobcat', 'powering', 'sudanese', 'sudanese', 'krystal', 'hoboken', 'avocados', 'gale']\n",
      "  sample 2:\n",
      "    input     > ['quatre', 'asked', 'with', 'a', 'smile', 'on', 'his', 'face', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['yzak', 'nodded', 'lightly', ',', 'watching', 'the', 'cup', 'of', 'tea', 'he', 'still', 'touched', 'with', 'his', 'fingertips', '.', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['rearview', 'rearview', 'puzzles', 'puzzles', 'puzzles', 'insurmountable', '/i', 'flagging', 'vigorously', '/i', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['so', 'i', 'stayed', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['we', 'went', 'to', 'a', 'little', 'pub', 'to', 'start', 'drinking', 'before', 'the', 'club', 'opened', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['mannie', 'mannie', 'mannie', 'emergency', '06', 'painters', 'urgency', 'resurfacing', 'revelation', 'revelation', 'outrageously', 'panama', 'vinnie', 'vinnie', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "360\n",
      "390\n",
      "420\n",
      "450\n",
      "480\n",
      "510\n",
      "540\n",
      "570\n",
      "600\n",
      "630\n",
      "660\n",
      "690\n",
      "720\n",
      "750\n",
      "780\n",
      "810\n",
      "840\n",
      "870\n",
      "900\n",
      "930\n",
      "960\n",
      "990\n",
      "batch 1000\n",
      "  minibatch loss: 6.318912029266357\n",
      "  sample 1:\n",
      "    input     > ['amy', 'made', 'some', 'bagels', 'with', 'cream', 'cheese', 'and', 'tomatoes', 'and', 'we', 'settled', 'down', 'for', 'a', 'snack', '.', 'PAD', 'PAD']\n",
      "    input     > ['mtv', 'news', 'was', 'on', 'the', 'tube', 'and', 'we', 'soon', 'heard', 'the', 'news', 'of', 'garcia', \"'s\", 'death', '.', 'PAD', 'PAD']\n",
      "    predicted > ['i', 'was', 'EOS', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['gawd', ',', 'i', \"'ve\", 'been', 'so', 'into', 'UNK', 'music', 'lately', '...', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['i', 'think', 'i', \"'m\", 'morphing', 'from', 'my', 'UNK', 'into', 'this', 'new', 'UNK', 'UNK', 't', 'life', '.', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['i', 'was', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['so', 'much', 'arousal', 'and', 'fear', 'all', 'day', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['my', 'body', 'was', 'so', 'primed', ',', 'pulled', 'so', 'tight', 'the', 'lightest', 'touch', 'was', 'almost', 'painful', '.', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['i', 'was', 'EOS', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "1020\n",
      "1050\n",
      "1080\n",
      "1110\n",
      "1140\n",
      "1170\n",
      "1200\n",
      "1230\n",
      "1260\n",
      "1290\n",
      "1320\n",
      "1350\n",
      "1380\n",
      "1410\n",
      "1440\n",
      "1470\n",
      "1500\n",
      "1530\n",
      "1560\n",
      "1590\n",
      "1620\n",
      "1650\n",
      "1680\n",
      "1710\n",
      "1740\n",
      "1770\n",
      "1800\n",
      "1830\n",
      "1860\n",
      "1890\n",
      "1920\n",
      "1950\n",
      "1980\n",
      "batch 2000\n",
      "  minibatch loss: 6.361737251281738\n",
      "  sample 1:\n",
      "    input     > ['what', 'you', 'might', 'become', 'is', 'a', 'mystery', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['piece', 'by', 'piece', ',', 'these', 'things', 'come', 'together', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['i', 'was', 'EOS', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['despite', 'what', 'transpired', 'last', 'saturday', ',', 'the', 'short-lived', 'concert', 'was', 'a', 'success', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['it', 'was', 'so', 'heartwarming', 'how', 'each', 'and', 'every', 'one', 'who', 'attended', 'were', 'all', 'smiles', ',', 'just', 'singing', 'or', 'humming', 'along', 'with', 'ely', 'the', 'whole', 'time', '.', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['i', 'was', 'EOS', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['for', 'individuals', 'who', 'have', 'diabetes', ',', 'the', 'treatment', 'goals', 'for', 'a', 'diabetes', 'diet', 'are', ':', 'attain', 'and', 'maintain', 'near', 'normal', 'blood', 'glucose', 'levels', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['to', 'maintain', 'your', 'diabetes', 'diet', ',', 'it', 'helps', 'to', 'determine', 'potential', 'barriers', 'and', 'the', 'means', 'to', 'break', 'through', 'them', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['i', 'was', 'EOS', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "2010\n",
      "2040\n",
      "2070\n",
      "2100\n",
      "2130\n",
      "2160\n",
      "2190\n",
      "2220\n",
      "2250\n",
      "2280\n",
      "2310\n",
      "2340\n",
      "2370\n",
      "2400\n",
      "2430\n",
      "2460\n",
      "2490\n",
      "2520\n",
      "2550\n",
      "2580\n",
      "2610\n",
      "2640\n",
      "2670\n",
      "2700\n",
      "2730\n",
      "2760\n",
      "2790\n",
      "2820\n",
      "2850\n",
      "2880\n",
      "2910\n",
      "2940\n",
      "2970\n",
      "3000\n",
      "batch 3000\n",
      "  minibatch loss: 5.159350395202637\n",
      "  sample 1:\n",
      "    input     > ['sections', 'started', 'to', 'cheer', ',', 'or', 'talk', 'amongst', 'themselves', '.', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['he', 'ignored', 'them', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['i', ',', '.', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 2:\n",
      "    input     > ['unfortunate', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    input     > ['UNK', 'her', \"'\", '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['i', 'was', 'EOS', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "  sample 3:\n",
      "    input     > ['i', 'love', 'people', 'who', 'will', 'hug', 'you', 'right', 'when', 'they', 'first', 'meet', 'you', '.']\n",
      "    input     > ['it', \"'s\", 'nice', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "    predicted > ['i', 'was', 'EOS', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "3030\n",
      "3060\n",
      "3090\n",
      "3120\n",
      "3150\n",
      "3180\n",
      "3210\n",
      "3240\n",
      "training interrupted\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "loss_track = []\n",
    "\n",
    "batches_in_epoch = 1000\n",
    "v, iv = data.vocab()\n",
    "iv[0]='PAD'\n",
    "iv[1]='EOS'\n",
    "iv[2]='UNK'\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed2(batch)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        if(batch%30==0):\n",
    "            print(batch)\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp1, inp2, pred) in enumerate(zip(fd[encoder_inputs].T, fd[encoder_inputs2].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format([iv[x] for x in inp1]))\n",
    "                print('    input     > {}'.format([iv[x] for x in inp2]))\n",
    "                print('    predicted > {}'.format([iv[x] for x in pred]))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 5.5017 after Tensor(\"mul:0\", shape=(), dtype=int32) examples (batch_size=Tensor(\"unstack:1\", shape=(), dtype=int32))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJh4v8BHgBNEnQs/6sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd33c69e198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "State should be a sequence or tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-6051be2ae657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojected_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minitial_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEOS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokens_to_inputs_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# isinstance(projected_state.c, tf.Tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/nlds/people/shobhit/tensorflow-seq2seq-tutorials/tf_beam_decoder.py\u001b[0m in \u001b[0;36mbeam_decoder\u001b[0;34m(cell, beam_size, stop_token, initial_state, initial_input, tokens_to_inputs_fn, outputs_to_score_fn, score_upper_bound, max_len, cell_transform, output_dense, scope)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mcell_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvarscope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         )\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/nlds/people/shobhit/tensorflow-seq2seq-tutorials/tf_beam_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, beam_size, stop_token, initial_state, initial_input, score_upper_bound, max_len, outputs_to_score_fn, tokens_to_inputs_fn, cell_transform, scope)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeamFlattenWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile_along_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile_along_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcell_transform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'replicate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeamReplicateWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/nlds/people/shobhit/tensorflow-seq2seq-tutorials/tf_beam_decoder.py\u001b[0m in \u001b[0;36mtile_along_beam\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State should be a sequence or tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: State should be a sequence or tensor"
     ]
    }
   ],
   "source": [
    "from tf_beam_decoder import beam_decoder\n",
    "decoded_sparse, decoded_logprobs = beam_decoder(\n",
    "    cell=decoder_cell,\n",
    "    beam_size=7,\n",
    "    stop_token=PAD,\n",
    "    initial_state=projected_state.c,\n",
    "    initial_input=EOS,\n",
    "    tokens_to_inputs_fn=lambda tokens: tf.nn.embedding_lookup(embeddings, tokens),\n",
    ")\n",
    "# isinstance(projected_state.c, tf.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Size must be provided if handle is not provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5121a7504470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0memit_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mdecoder_outputs_ta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_final_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mdecoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_outputs_ta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-5121a7504470>\u001b[0m in \u001b[0;36mcustom_rnn\u001b[0;34m(cell, loop_fn, parallel_iterations, swap_memory, scope)\u001b[0m\n\u001b[1;32m     56\u001b[0m     (finished, next_input, initial_state, _, loop_state) = loop_fn(\n\u001b[1;32m     57\u001b[0m         time=time, previous_output=None, previous_state=None, previous_loop_state=None, k=0)\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0memit_ta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/nlds/people/shobhit/tensorflow-seq2seq-tutorials/venv/lib/python3.4/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dtype, size, dynamic_size, clear_after_read, tensor_array_name, handle, flow, infer_shape, element_shape, name)\u001b[0m\n\u001b[1;32m    133\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Handle must be a Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Size must be provided if handle is not provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       raise ValueError(\"Cannot provide both a handle and size \"\n",
      "\u001b[0;31mValueError\u001b[0m: Size must be provided if handle is not provided"
     ]
    }
   ],
   "source": [
    "beam_size=5\n",
    "\n",
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state, k):\n",
    "    \n",
    "    def get_next_input():\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        \n",
    "        best_probs, indices = tf.nn.top_k(logits, beam_size)\n",
    "        prediction = indices[-1]#TODO\n",
    "        \n",
    "        assert tf.argmax(output_logits, axis=1) == indices[0]\n",
    "        \n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)\n",
    "def loop_fn(time, previous_output, previous_state, previous_loop_state, k):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state, k)\n",
    "\n",
    "def custom_rnn(cell,\n",
    "    loop_fn,\n",
    "    parallel_iterations=None,\n",
    "    swap_memory=False,\n",
    "    scope=None):\n",
    "    \n",
    "    time = tf.constant(0, dtype=tf.int32)\n",
    "    (finished, next_input, initial_state, _, loop_state) = loop_fn(\n",
    "        time=time, previous_output=None, previous_state=None, previous_loop_state=None, k=0)\n",
    "    emit_ta = tf.TensorArray(dynamic_size=True, dtype=initial_state.dtype)\n",
    "    state = initial_state\n",
    "    while not all(finished):\n",
    "        (output, cell_state) = cell(next_input, state)\n",
    "        \n",
    "        \n",
    "        (next_finished, next_input, next_state, emit, loop_state) = loop_fn(\n",
    "            time=time + 1, previous_output=output, previous_state=cell_state,\n",
    "            previous_loop_state=loop_state, k=0)\n",
    "        \n",
    "        # Emit zeros and copy forward state for minibatch entries that are finished.\n",
    "        state = tf.where(finished, state, next_state)\n",
    "        emit = tf.where(finished, tf.zeros_like(emit), emit)\n",
    "        emit_ta = emit_ta.write(time, emit)\n",
    "        # If any new minibatch entries are marked as finished, mark these.\n",
    "        finished = tf.logical_or(finished, next_finished)\n",
    "        time += 1\n",
    "    return (emit_ta, state, loop_state)\n",
    "    \n",
    "decoder_outputs_ta, decoder_final_state, _ = custom_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()\n",
    "\n",
    "    \n",
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n",
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=10,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)\n",
    "def add(batch1, batch2):\n",
    "    targetSeq = []\n",
    "    for i in range(0, max(len(batch1), len(batch2))):\n",
    "        if i >=len(batch1):\n",
    "            targetSeq.append((int)(batch2[i]/2))\n",
    "        elif i >=len(batch2):\n",
    "            targetSeq.append((int)(batch1[i])/2)\n",
    "        else:\n",
    "            targetSeq.append((int)((batch1[i]+batch2[i])/2))\n",
    "    return targetSeq\n",
    "def next_feed():\n",
    "    batch1 = next(batches)\n",
    "    batch2 = next(batches)\n",
    "    encoder_inputs_1, encoder_input_lengths_1 = helpers.batch(batch1)\n",
    "    encoder_inputs_2, encoder_input_lengths_2 = helpers.batch(batch2)\n",
    "    \n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "    [add(x,y)  + [EOS] + [PAD] * 2 for x,y in zip(batch1, batch2)]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_1,\n",
    "        encoder_inputs_length: encoder_input_lengths_1,\n",
    "        encoder_inputs2: encoder_inputs_2,\n",
    "        encoder_inputs_length2: encoder_input_lengths_2,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }\n",
    "loss_track = []\n",
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "\n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp1, inp2, pred) in enumerate(zip(fd[encoder_inputs].T, fd[encoder_inputs2].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp1))\n",
    "                print('    input     > {}'.format(inp2))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
